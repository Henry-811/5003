---
title: "Travel Mode Prediction for Sustainable Transport Planning"
subtitle: "Gold Coast Household Travel Survey Analysis"
author: "STAT5003 - Week 11, Group 07"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: false
    transition: faster
    css: custom.css
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align = "center")

# Load required libraries
library(dplyr)
library(ggplot2)
library(knitr)
library(readr)
library(tidyr)
library(scales)

# Set theme for all plots
theme_set(theme_minimal(base_size = 14))
```

## Problem Statement {.build}

**Objective:** Predict travel mode choice on the Gold Coast using household survey data

**Task:** Multi-class classification

- Vehicle Driver / Passenger
- Public Transport (Train, Bus, Ferry)
- Active Transport (Walking, Bicycle)

**Why it matters:**

- Support sustainable transport planning
- Inform infrastructure investment decisions
- Enable targeted policy interventions

## Initial Dataset

<div class="columns-2" style="font-size: 24px; line-height: 1.8;">

<div>
<div style="font-size: 24px; font-weight: 700; color: #E74C3C; margin-bottom: 15px;">Dataset Overview</div>

- <span style="color: #E74C3C; font-weight: 700;">Source:</span> 2015 Gold Coast Survey
- <span style="color: #E74C3C; font-weight: 700;">Size:</span> 22,587 observations
- <span style="color: #E74C3C; font-weight: 700;">Variables:</span> 179 → 163
</div>

<div>
<div style="font-size: 24px; font-weight: 700; color: #E74C3C; margin-bottom: 15px;">Key Challenges Identified</div>

- <span style="color: #E74C3C; font-weight: 700;">Missing values:</span> Up to 99%
- <span style="color: #E74C3C; font-weight: 700;">Class imbalance:</span> 80%+ vehicle
- <span style="color: #E74C3C; font-weight: 700;">Outliers:</span> 20+ variables >1%
- <span style="color: #E74C3C; font-weight: 700;">Data leakage:</span> Vehicle attributes
</div>

</div>

## Exploratory Data Analysis

```{r week7-class-distribution, fig.height=5.5, fig.width=10}
# Actual class distribution from Week 7 dataset (STOP_MAINMODE)
mode_data <- data.frame(
  Mode = c("Vehicle Driver", "Vehicle Passenger", "Walking", "Bicycle",
           "Public Bus", "School Bus", "Train", "Other",
           "Ferry", "Motorcycle", "Taxi", "Plane", "Other Bus"),
  Count = c(13176, 5146, 2919, 303, 247, 223, 194, 164, 79, 73, 42, 13, 8),
  Percentage = c(58.30, 22.80, 12.90, 1.30, 1.10, 1.00, 0.90, 0.70, 0.30, 0.30, 0.20, 0.10, 0.04)
)

ggplot(mode_data, aes(x = reorder(Mode, -Count), y = Count, fill = Mode)) +
  geom_col(width = 0.7) +
  geom_text(aes(label = paste0(Percentage, "%")), vjust = -0.5, size = 3.5, fontface = "bold") +
  scale_fill_manual(values = rep(RColorBrewer::brewer.pal(12, "Set3"), length.out = 13)) +
  scale_y_continuous(limits = c(0, 14000), expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Travel Mode Distribution (All 13 Classes)",
       x = "Mode", y = "Number of Trips") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        axis.text.y = element_text(size = 11),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 15, face = "bold"),
        legend.position = "none",
        plot.margin = margin(5, 20, 5, 10))
```

<div style="font-size: 18px; margin-top: -10px;">
**Key Finding:** Severe class imbalance - dominant vehicle usage requires careful handling
</div>
## EDA: Feature-Target Correlation

```{r eda-correlation, out.width="70%", fig.align="center"}
knitr::include_graphics("figures/3c3e076a79850f23c0094572ba325240.png")
```

<div style="font-size: 18px; margin-top: 10px;">
**Finding:** Vehicle features (VEH_VEHYEAR, STOP_NETWORK_DIST) show strongest correlation with travel mode
</div>
## EDA: Categorical Features

```{r eda-categorical, out.width="70%", fig.align="center"}
knitr::include_graphics("figures/d172ed278a286e7a685cdc0525ae8745.png")
```

<div style="font-size: 18px; margin-top: 10px;">
**Finding:** STOP_FULLMODE, VEH_RUNNINGCOST, PERS_NOLICENCE strongly associated with mode choice
</div>
## EDA: Feature Collinearity

```{r eda-collinearity, out.width="70%", fig.align="center"}
knitr::include_graphics("figures/ba6cbae66ca1b8a816d387e535619f89.png")
```

<div style="font-size: 18px; margin-top: 10px;">
**Finding:** High multicollinearity detected among vehicle attributes (hierarchical clustering)
</div>
## Key Findings from EDA

**Top Predictive Features (from EDA)**

- **Numerical (η²):** Vehicle characteristics dominated
  - `VEH_VEHYEAR`, `STOP_NETWORK_DIST`, `VEH_HHWGT14`

- **Categorical (Cramér's V):** Strong associations found
  - `STOP_FULLMODE` (1.0 - data leakage!)
  - `PERS_NOLICENCE`, `VEH_RUNNINGCOST`

**Key Insight:** Vehicle proxies dominated predictions → Need to remove for real-world applicability

## Feature Selection & Engineering

<div style="font-size: 18px; line-height: 1.7;">

<div style="background-color: #ECF0F1; padding: 15px; margin-bottom: 20px; border-left: 5px solid #3498DB;">
<span style="color: #E74C3C; font-weight: 700; font-size: 18px;">EDA Insight:</span>
<span style="font-size: 18px;">Vehicle attributes dominated predictions - these are direct proxies that reveal the answer</span>
</div>

<div style="margin-top: 20px; margin-bottom: 15px;">
<span style="color: #E74C3C; font-weight: 700; font-size: 20px;">Our Approach:</span>
</div>

<div style="margin-top: 10px;">

**❌ Removed 81 variables (163 → 92 columns)**

- 25 vehicle proxies (VEH_VEHTYPE, fuel types - reveal the answer)
- 54 administrative variables (person IDs, timestamps - don't affect travel choice)
- 2 low-value variables (94% identical values - no variation to learn from)

**➕ Added 9 engineered features**

- VEH_PER_PERSON, VEH_PER_ADULT (household vehicle availability)
- TRIP_SPEED, TIME_PERIOD, IS_WEEKEND (trip timing characteristics)
- DIST_CATEGORY, HAS_LICENSE_AND_CAR (travel capability indicators)

</div>

</div>
## Dataset Transformation

```{r preprocessing-summary, fig.height=4.5, fig.width=10}
# Before/After comparison - focus on data leakage removal
preprocessing_data <- data.frame(
  Dataset = rep(c("Initial Dataset", "After Cleaning"), each = 2),
  Category = rep(c("Total Variables", "Vehicle Proxies"), 2),
  Count = c(163, 25,  # Before: 163 total, 25 proxies
            92, 0)     # After: 92 total, 0 proxies (removed 81 total, added 9)
)

ggplot(preprocessing_data, aes(x = Category, y = Count, fill = Dataset)) +
  geom_col(position = "dodge", width = 0.7) +
  geom_text(aes(label = Count), position = position_dodge(width = 0.7),
            vjust = -0.5, size = 5, fontface = "bold") +
  scale_fill_manual(values = c("Initial Dataset" = "#E74C3C",
                                 "After Cleaning" = "#27AE60")) +
  scale_y_continuous(limits = c(0, 180), expand = expansion(mult = c(0, 0.05))) +
  labs(title = "Dataset Transformation: Removing Data Leakage",
       y = "Number of Variables", x = NULL) +
  theme(legend.position = "top",
        legend.title = element_blank(),
        legend.text = element_text(size = 14),
        axis.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        plot.title = element_text(size = 16, face = "bold"))
```

<div style="font-size: 16px; color: #7F8C8D; margin-top: -5px;">
*Chart shows vehicle proxy removal; also removed 56 variables irrelevant to travel choice (survey IDs, timestamps, low-variance)*
</div>

<div style="font-size: 18px; margin-top: 10px;">
<span style="color: #E74C3C; font-weight: 700; font-size: 18px;">Key Distinction:</span> <span style="font-size: 18px;">Kept `HH_CARS` (availability) ✓ | Removed `VEH_VEHTYPE` (usage) ✗</span>
</div>
## Model Selection

<div style="font-size: 18px; line-height: 1.7;">

**These are 5 models we choose:** Random Forest, XGBoost, GLM-NET, SVM, Decision Tree

<div style="margin-top: 20px;">

**Why These 5 Models?**

<div style="margin-top: 15px;">

- **Diverse Approaches:** Tree-based (RF, XGBoost, DT), Linear (GLM-NET), Kernel-based (SVM)
- **Handle Class Imbalance:** All support class weighting or rebalancing techniques
- **Interpretability vs Performance:** From simple (Decision Tree) to complex (XGBoost)
- **Industry Standard:** Widely used for multi-class classification in transport research

</div>

</div>

**Common Setup**

- Same 92-column policy-focused dataset
- 80/20 train/test split with stratified sampling
- 5-fold cross-validation
- Macro-F1 optimization (equal weight to all modes)

</div>
## Model Implementation

<div style="font-size: 22px; line-height: 1.8;">

**5 Classification Models Trained**

<div style="margin-top: 25px;">

- **Random Forest:** Grid search (mtry, min_n, trees) with class weighting
- **XGBoost:** Gradient boosting with Bayesian optimization (22 iterations)
- **GLM-NET:** Elastic net regularization with Bayesian optimization
- **SVM:** RBF kernel with Grid search (cost, gamma)
- **Decision Tree:** Bayesian optimization (22 iterations) with complexity tuning

</div>

**Evaluation Strategy**

<div style="margin-top: 15px;">

- **Macro-F1 Score:** Handles class imbalance by giving equal weight to all modes
- **5-fold Cross-Validation:** Ensures robust performance estimates
- **Stratified Sampling:** Maintains class distribution across folds

</div>

</div>
## Model Performance Comparison

```{r model-performance, fig.height=5, fig.width=10}
# ACTUAL model performance results from optimization logs
model_results <- data.frame(
  Model = c("Random Forest", "XGBoost", "GLM-NET", "SVM", "Decision Tree"),
  Accuracy = c(0.9258, 0.9399, 0.9052, 0.7702, 0.8862),
  `Macro-F1` = c(0.7153, 0.8476, 0.6225, 0.5031, 0.4311),
  `Macro-Precision` = c(0.9410, 0.8718, 0.7297, 0.5762, 0.4462),
  `Macro-Recall` = c(0.6147, 0.8527, 0.5890, 0.2438, 0.4266),
  check.names = FALSE
)

# Reshape for plotting
model_metrics <- model_results %>%
  pivot_longer(cols = c(Accuracy, `Macro-F1`, `Macro-Precision`, `Macro-Recall`),
               names_to = "Metric",
               values_to = "Score")

# Grouped by Metric - x-axis = Metric, colored bars = Models
ggplot(model_metrics, aes(x = Metric, y = Score, fill = Model)) +
  geom_col(position = "dodge", width = 0.75) +
  geom_text(aes(label = sprintf("%.2f", Score)),
            position = position_dodge(width = 0.75),
            vjust = -0.3, size = 3.3, fontface = "bold") +
  scale_fill_manual(values = c("XGBoost" = "#E67E22",
                                 "Random Forest" = "#3498DB",
                                 "GLM-NET" = "#9B59B6",
                                 "SVM" = "#E74C3C",
                                 "Decision Tree" = "#1ABC9C")) +
  scale_y_continuous(limits = c(0, 1.05),
                     labels = function(x) sprintf("%.2f", x),
                     expand = expansion(mult = c(0, 0)),
                     breaks = seq(0, 1, 0.25)) +
  labs(x = NULL, y = "Score") +
  theme(legend.position = "top",
        legend.title = element_blank(),
        legend.text = element_text(size = 13),
        axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 11),
        axis.title.y = element_text(size = 12),
        plot.margin = margin(5, 15, 5, 5))
```

<div style="font-size: 18px; margin-top: 5px;">
<span style="color: #E74C3C; font-weight: 700; font-size: 18px;">Key Insight:</span> <span style="font-size: 18px;">XGBoost (orange) best overall balance; Random Forest (blue) highest precision</span>
</div>
## Model Details: XGBoost

<div style="display: flex; font-size: 20px; line-height: 1.7; gap: 40px;">

<div style="flex: 1;">
<div style="font-size: 20px; font-weight: 700; color: #E74C3C; margin-bottom: 15px;">Optimized Hyperparameters</div>

- **max_depth:** 15 (deep trees for complex patterns)
- **eta (learning rate):** 0.30 (moderate speed)
- **subsample:** 1.00 (use all data)
- **colsample_bytree:** 1.00 (use all features)
- **nrounds:** 100 (boosting iterations)
</div>

<div style="flex: 1;">
<div style="font-size: 20px; font-weight: 700; color: #E74C3C; margin-bottom: 15px;">Key Findings</div>

- **Balanced Performance:** High recall (0.85) and precision (0.87) across all modes
- **Handles Rare Classes:** Successfully predicts minority modes despite 80%+ vehicle dominance
- **Robust to Imbalance:** Macro-F1 of 0.85 confirms equal performance across classes
- **Excellent Generalization:** Test F1 (0.85) exceeds CV F1 (0.77), showing strong real-world applicability
- **Identifies Key Factors:** Distance (9.1%), speed (7.0%), and transit access (5.7%) are top predictors
</div>

</div>
## Evaluation & Limitations

<div style="font-size: 18px; line-height: 1.6;">

**Performance Analysis**

- **XGBoost achieves best balance:** F1=0.85, but Random Forest has higher precision (0.94 vs 0.87)
- **Why Macro-F1?** Accuracy alone misleading - would achieve 80%+ by always predicting "vehicle"
- **Strong generalization:** Test F1 (0.85) exceeds CV F1 (0.77) for XGBoost

**Limitations**

- **Insufficient data for rare modes:**
  - Ferry: 79 trips (0.3%), Plane: 13 trips (0.1%), Other Bus: 8 trips (0.0%)
  - Models struggle to learn patterns with so few samples

- **Real-world deployment challenges:**
  - Rare mode predictions may be unreliable in practice
  - Focus on major modes (Vehicle, Walking, Bus, Train) for policy decisions

</div>
## Conclusion

<div style="font-size: 20px; line-height: 1.7;">

**Key Takeaways:**

- **Data quality is critical:** Removing vehicle proxies improved real-world applicability
- **Class imbalance is manageable:** Proper techniques (weighting, Macro-F1) handle 80%+ majority class
- **Model selection matters:** XGBoost outperformed other models for multi-class imbalanced data

**Impact:** Feature importance reveals actionable patterns for targeted policy interventions:

- **Distance-based transit:** Top predictor (9.1%) → Optimize service for specific trip lengths
- **Speed competitiveness:** Critical factor (7.0%) → Ensure transit is time-competitive
- **Transit-oriented planning:** Infrastructure access matters (5.7%) → Focus improvements near stations

</div>

</div>
## Future Directions

<div style="font-size: 22px; line-height: 1.8;">

**Next Steps for Deployment:**

- **Geographic targeting:** Apply model to identify specific suburbs with high mode-shift potential
- **Scenario analysis:** Test proposed interventions (e.g., faster buses, new routes) using model predictions
- **Real-world partnership:** Collaborate with Gold Coast transport authority for implementation

**Model Enhancements:**

- Collect additional data for rare modes (ferry, plane, other bus) to improve predictions
- Incorporate temporal patterns (seasonal, day-of-week, time-of-day variations)
- Integrate real-time factors (traffic conditions, weather, service disruptions)

</div>
## Questions & Discussion {.flexbox .vcenter}

<div class="centered" style="margin-top: 100px;">

<div style="font-size: 48px; color: #2C3E50; font-weight: 700; margin-bottom: 40px;">
Thank You!
</div>

<div style="font-size: 36px; color: #3498DB; font-weight: 600;">
Any Questions?
</div>

</div>
## Appendix: Optimized Hyperparameters {.smaller}

<div style="font-size: 18px;">

**XGBoost (Best: Macro-F1 = 0.85)**
- `max_depth`: 15, `eta`: 0.30, `subsample`: 1.00, `colsample_bytree`: 1.00

**Random Forest (Best: Macro-F1 = 0.72)**
- `mtry`: 18, `min_n`: 2, `trees`: 600

**GLM-NET (Best: Macro-F1 = 0.62)**
- `penalty`: 0.000013, `mixture`: 0.36 (more L2 regularization)

**SVM (Best: Macro-F1 = 0.50)**
- `cost`: 4.00, `gamma`: 0.050, CV Macro-F1: 0.27 (±0.01)

**Decision Tree (Best: Macro-F1 = 0.43)**
- `cp`: 0.0001, `max_depth`: 22, `min_split`: 10, `min_bucket`: 25

</div>

**Note:** All models used 5-fold cross-validation for hyperparameter tuning